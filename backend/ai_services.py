import os
import asyncio
import aiohttp
import json
from typing import Optional, Dict, List
from dotenv import load_dotenv

try:
    import google.generativeai as genai  # type: ignore
except Exception:
    genai = None  # type: ignore

load_dotenv()

class AIServiceProvider:
    """Service provider for AI integrations with Gemini (primary), Groq (fallback), and Hugging Face."""
    
    def __init__(self):
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        self.hf_api_key = os.getenv("HUGGINGFACE_API_KEY")
        self.gemini_api_key = os.getenv("GOOGLE_API_KEY")
        
        # API endpoints
        self.groq_base_url = "https://api.groq.com/openai/v1"
        self.hf_base_url = "https://api-inference.huggingface.co/models"
        
        # Model configurations
        # TTS via Groq
        self.tts_model_groq = "playai-tts"
        self.tts_voice_groq = "Basil-PlayAI"
        self.tts_response_format = "wav"
        # TTS via Hugging Face (fallback)
        # Commonly available TTS model that returns audio bytes via Inference API
        self.hf_tts_model = "espnet/kan-bayashi_ljspeech_vits"
        # STT via Hugging Face
        self.stt_model = "openai/whisper-large-v3"
        # LLMs
        # Primary: Gemini 2.0 Flash
        self.gemini_model = os.getenv("GEMINI_LLM_MODEL", "gemini-2.0-flash")
        # Fallback: Groq LLM
        self.llm_model = os.getenv("GROQ_LLM_MODEL", "llama-3.1-8b-instant")

    # ----------------------
    # Gemini helpers
    # ----------------------
    async def _gemini_generate_text(self, prompt: str, model: Optional[str] = None) -> str:
        if not self.gemini_api_key or genai is None:
            raise RuntimeError("Gemini not configured")
        def _call() -> str:
            genai.configure(api_key=self.gemini_api_key)
            m = genai.GenerativeModel(model or self.gemini_model)
            resp = m.generate_content(prompt)
            try:
                return resp.text  # type: ignore[attr-defined]
            except Exception:
                return str(resp)
        return await asyncio.to_thread(_call)

    async def generate_question_with_gemini(self, domain: str, difficulty: str = "intermediate", previous_questions: Optional[List[str]] = None) -> Dict:
        """Generate an interview question using Gemini."""
        if not self.gemini_api_key or genai is None:
            return {"success": False, "error": "Gemini not configured"}
        prev_q_text = ""
        if previous_questions:
            prev_q_text = "\n\nPrevious questions asked:\n" + "\n".join(f"- {q}" for q in previous_questions)
            prev_q_text += "\n\nMake sure the new question is different and progressive."
        prompt = f"""You are an expert technical interviewer for {domain}.
Generate 1 {difficulty} level interview question for {domain} domain.

Requirements:
- The question should be technical and relevant to {domain}
- Difficulty level: {difficulty}
- The question should allow for detailed explanations
- Avoid yes/no questions
- Focus on concepts, algorithms, or practical applications{prev_q_text}

Return only the question, no additional text."""
        try:
            text = await self._gemini_generate_text(prompt)
            question = (text or "").strip().strip('"')
            print(f"[AI] Question generated by: {self.gemini_model}")
            return {"success": True, "question": question, "model": self.gemini_model, "domain": domain, "difficulty": difficulty}
        except Exception as e:
            return {"success": False, "error": f"Gemini error: {e}"}
    
    async def generate_question_with_groq(self, domain: str, difficulty: str = "intermediate", 
                                        previous_questions: List[str] = None) -> Dict:
        """Generate interview questions using Groq LLM"""
        try:
            if not self.groq_api_key:
                raise ValueError("Groq API key not configured")
            
            # Create prompt based on domain and difficulty
            prev_q_text = ""
            if previous_questions:
                prev_q_text = f"\n\nPrevious questions asked:\n" + "\n".join(f"- {q}" for q in previous_questions)
                prev_q_text += "\n\nMake sure the new question is different and progressive."
            
            prompt = f"""You are an expert technical interviewer for {domain}. 
Generate 1 {difficulty} level interview question for {domain} domain.

Requirements:
- The question should be technical and relevant to {domain}
- Difficulty level: {difficulty}
- The question should allow for detailed explanations
- Avoid yes/no questions
- Focus on concepts, algorithms, or practical applications{prev_q_text}

Return only the question, no additional text."""

            headers = {
                "Authorization": f"Bearer {self.groq_api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": self.llm_model,
                "messages": [
                    {"role": "system", "content": "You are a technical interviewer."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 200
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.groq_base_url}/chat/completions",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        question = data["choices"][0]["message"]["content"].strip()
                        print(f"[AI] Question generated by: {self.llm_model}")
                        return {
                            "question": question,
                            "model": self.llm_model,
                            "domain": domain,
                            "difficulty": difficulty,
                            "success": True
                        }
                    else:
                        error_text = await response.text()
                        return {
                            "error": f"Groq API error: {error_text}",
                            "success": False
                        }
                        
        except Exception as e:
            return {
                "error": f"Question generation failed: {str(e)}",
                "success": False
            }

    async def generate_question(self, domain: str, difficulty: str = "intermediate", previous_questions: Optional[List[str]] = None) -> Dict:
        """Primary Gemini, fallback to Groq."""
        # Try Gemini first
        res = await self.generate_question_with_gemini(domain, difficulty, previous_questions)
        if res.get("success"):
            return res
        # Fallback to Groq
        res2 = await self.generate_question_with_groq(domain, difficulty, previous_questions or [])
        return res2
    
    async def evaluate_answer_with_groq(self, question: str, answer: str, domain: str) -> Dict:
        """Evaluate interview answer using Groq LLM"""
        try:
            if not self.groq_api_key:
                raise ValueError("Groq API key not configured")
            
            prompt = f"""You are an expert technical interviewer evaluating answers for {domain} domain.

Question: {question}

Candidate's Answer: {answer}

Please evaluate this answer and provide:
1. A score from 1-10 (10 being excellent)
2. Detailed feedback on the answer quality
3. Specific suggestions for improvement
4. What the candidate did well

Format your response as JSON:
{{
    "score": <number>,
    "feedback": "<detailed feedback>",
    "strengths": ["<strength1>", "<strength2>"],
    "improvements": ["<improvement1>", "<improvement2>"]
}}"""

            headers = {
                "Authorization": f"Bearer {self.groq_api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": self.llm_model,
                "messages": [
                    {"role": "system", "content": "You are a technical interviewer and evaluator."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3,
                "max_tokens": 500
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.groq_base_url}/chat/completions",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        result = data["choices"][0]["message"]["content"].strip()
                        
                        # Parse JSON response
                        try:
                            evaluation = json.loads(result)
                            evaluation["success"] = True
                            evaluation["model"] = self.llm_model
                            print(f"[AI] Evaluation by: {self.llm_model}")
                            return evaluation
                        except json.JSONDecodeError:
                            # Fallback if JSON parsing fails
                            eval_obj = {
                                "score": 7.0,
                                "feedback": result,
                                "strengths": ["Answer provided"],
                                "improvements": ["Could be more detailed"],
                                "success": True,
                                "model": self.llm_model
                            }
                            print(f"[AI] Evaluation by: {self.llm_model}")
                            return eval_obj
                    else:
                        error_text = await response.text()
                        return {
                            "error": f"Groq API error: {error_text}",
                            "success": False
                        }
                        
        except Exception as e:
            return {
                "error": f"Answer evaluation failed: {str(e)}",
                "success": False
            }

    async def evaluate_answer_with_gemini(self, question: str, answer: str, domain: str) -> Dict:
        """Evaluate interview answer using Gemini, requesting JSON format."""
        if not self.gemini_api_key or genai is None:
            return {"success": False, "error": "Gemini not configured"}
        prompt = f"""You are an expert technical interviewer evaluating answers for {domain} domain.

Question: {question}

Candidate's Answer: {answer}

Please evaluate this answer and provide the following as strict JSON only (no extra text):
{{
  "score": <number 1-10>,
  "feedback": "<detailed feedback>",
  "strengths": ["<strength1>", "<strength2>"],
  "improvements": ["<improvement1>", "<improvement2>"]
}}
"""
        try:
            text = await self._gemini_generate_text(prompt)
            try:
                data = json.loads(text)
            except Exception:
                # If it didn't return pure JSON, wrap as feedback
                data = {
                    "score": 7.0,
                    "feedback": text,
                    "strengths": ["Answer provided"],
                    "improvements": ["Could be more detailed"],
                }
            data["success"] = True
            data["model"] = self.gemini_model
            print(f"[AI] Evaluation by: {self.gemini_model}")
            return data
        except Exception as e:
            return {"success": False, "error": f"Gemini error: {e}"}

    async def evaluate_answer(self, question: str, answer: str, domain: str) -> Dict:
        """Primary Gemini, fallback to Groq."""
        res = await self.evaluate_answer_with_gemini(question, answer, domain)
        if res.get("success"):
            return res
        res2 = await self.evaluate_answer_with_groq(question, answer, domain)
        return res2

    async def generate_preamble_with_groq(self, name: Optional[str], question: str, question_index: int, total_questions: int, domain: Optional[str]) -> Dict:
        """Generate a short, conversational preamble for a question using Groq LLM."""
        try:
            if not self.groq_api_key:
                raise ValueError("Groq API key not configured")

            safe_name = name or "there"
            position = "first" if question_index == 0 else ("last" if question_index == total_questions - 1 else "next")
            dom_txt = f" in the {domain} domain" if domain else ""
            # Keep it concise and human; do not include the question text itself, we append it on client
            prompt = f"""
You are a friendly, human interviewer. Write a very short preamble (1–2 sentences) that:
- Greets the candidate by name ({safe_name}).
- Acknowledges progress based on position: {position} of {total_questions}.
- Feels encouraging and conversational{dom_txt}.
- Does NOT restate or reveal the question content.
- Ends ready to read the question (e.g., "here it is:" or "let's dive in:").

Candidate name: {safe_name}
Question position: {question_index + 1} of {total_questions}
Domain: {domain or 'general'}
""".strip()

            headers = {
                "Authorization": f"Bearer {self.groq_api_key}",
                "Content-Type": "application/json"
            }

            payload = {
                "model": self.llm_model,
                "messages": [
                    {"role": "system", "content": "You are a warm, concise human interviewer."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.8,
                "max_tokens": 120
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.groq_base_url}/chat/completions",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data["choices"][0]["message"]["content"].strip()
                        # Trim overly long responses
                        if len(content) > 400:
                            content = content[:400].rstrip() + "…"
                        return {"preamble": content, "success": True, "model": self.llm_model}
                    else:
                        return {"success": False, "error": f"Groq API error: {await response.text()}"}
        except Exception as e:
            return {"success": False, "error": f"Preamble generation failed: {e}"}
    
    async def text_to_speech_hf(self, text: str) -> Dict:
        """Convert text to speech using Hugging Face TTS model"""
        try:
            if not self.hf_api_key:
                raise ValueError("Hugging Face API key not configured")
            
            headers = {
                "Authorization": f"Bearer {self.hf_api_key}",
                "Content-Type": "application/json",
            }
            
            payload = {
                "inputs": text
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.hf_base_url}/{self.hf_tts_model}",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        audio_bytes = await response.read()
                        # Convert to base64 for transmission
                        import base64
                        audio_b64 = base64.b64encode(audio_bytes).decode()
                        
                        return {
                            "audio_data": f"data:audio/wav;base64,{audio_b64}",
                            "model": self.hf_tts_model,
                            "success": True
                        }
                    else:
                        error_text = await response.text()
                        return {
                            "error": f"TTS API error: {error_text}",
                            "success": False
                        }
                        
        except Exception as e:
            return {
                "error": f"TTS conversion failed: {str(e)}",
                "success": False
            }

    async def text_to_speech_groq(self, text: str, voice: Optional[str] = None, response_format: Optional[str] = None) -> Dict:
        """Convert text to speech using Groq TTS (playai-tts). Returns base64 data URL.
        Enhancements: explicit Accept header, response_format validation, improved error parsing, optional HF fallback.
        """
        try:
            if not self.groq_api_key:
                raise ValueError("Groq API key not configured")

            fmt = (response_format or self.tts_response_format or "wav").lower()
            if fmt not in ("wav", "mp3"):
                fmt = "wav"
            accept = "audio/wav" if fmt == "wav" else "audio/mpeg"

            headers = {
                "Authorization": f"Bearer {self.groq_api_key}",
                "Content-Type": "application/json",
                "Accept": accept,
            }

            payload = {
                "model": self.tts_model_groq,
                "voice": voice or self.tts_voice_groq,
                "response_format": fmt,
                "input": text,
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.groq_base_url}/audio/speech",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        audio_bytes = await response.read()
                        import base64
                        audio_b64 = base64.b64encode(audio_bytes).decode()
                        return {
                            "audio_data": f"data:{accept};base64,{audio_b64}",
                            "model": self.tts_model_groq,
                            "voice": payload["voice"],
                            "success": True
                        }
                    else:
                        # Try to parse JSON error if present
                        try:
                            data = await response.json()
                            msg = data.get("error") or data
                        except Exception:
                            msg = await response.text()
                        return {
                            "error": f"Groq TTS API error ({response.status}): {msg}",
                            "success": False
                        }
        except Exception as e:
            # Optional lightweight fallback to HF if available
            try:
                hf_result = await self.text_to_speech_hf(text)
                if hf_result.get("success"):
                    hf_result["note"] = f"Groq TTS failed; fallback used: {str(e)}"
                    return hf_result
            except Exception:
                pass
            return {
                "error": f"Groq TTS conversion failed: {str(e)}",
                "success": False
            }
    
    async def speech_to_text_hf(self, audio_bytes: bytes, content_type: Optional[str] = None) -> Dict:
        """Convert speech to text using Hugging Face STT model (openai/whisper-large-v3).
        Optionally forward the incoming content-type (e.g., audio/webm, audio/wav).
        """
        try:
            if not self.hf_api_key:
                raise ValueError("Hugging Face API key not configured")
            
            headers = {
                "Authorization": f"Bearer {self.hf_api_key}",
            }
            if content_type:
                headers["Content-Type"] = content_type
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.hf_base_url}/{self.stt_model}",
                    headers=headers,
                    data=audio_bytes
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        
                        # Different models return different formats
                        transcription = ""
                        if isinstance(result, dict):
                            transcription = result.get("text", str(result))
                        elif isinstance(result, list) and len(result) > 0:
                            transcription = result[0].get("generated_text", str(result[0]))
                        else:
                            transcription = str(result)
                        
                        return {
                            "transcription": transcription,
                            "model": self.stt_model,
                            "confidence": 0.9,  # HF doesn't always provide confidence
                            "success": True
                        }
                    else:
                        error_text = await response.text()
                        return {
                            "error": f"STT API error: {error_text}",
                            "success": False
                        }
                        
        except Exception as e:
            return {
                "error": f"STT conversion failed: {str(e)}",
                "success": False
            }

# Singleton instance
ai_service = AIServiceProvider()